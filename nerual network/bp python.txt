#8、训练BP网络
    def train(self, X, y, r, Iters):
        self.n_samples, self.n_features =np.shape(X)
        self.trainSet = X
        self.label = y.reshape(-1,1)
        #6.1 初始化网络权值
        for i in range(self.hiddenslayers+1):
            if i==0:
                #输入层
                W_xinput = self.initW(self.n_features, self.hiddenNeurons[i])
            elif i==self.hiddenslayers:
                #输出层
                w_hidden = self.initW(self.hiddenNeurons[i-1], self.label.shape[1])
                hi_layer = net(self.hiddenNeurons[i-1], w_hidden, "sigmoid")
                self.hiddens.append(hi_layer)
            else:
                #隐藏层
                w_hidden = self.initW(self.hiddenNeurons[i-1], self.hiddenNeurons[i])
                hi_layer = net(self.hiddenNeurons[i-1], w_hidden, "sigmoid")
                self.hiddens.append(hi_layer)
        #------------------------------------------------------------------------
        for i in range(Iters):
            #6.2 正向计算输出值
            yi_output = self.calResult(W_xinput, self.hiddens, self.trainSet)
            #6.3 计算整体误差并保存
            SSE = self.errorfunc(self.label, yi_output)
            if SSE<self.tol:
                break
            self.errList.append(SSE[0])
            #6.4 反向传播误差计算梯度并更新权重
            W_input = self.bwPropagation(W_xinput, self.hiddens, self.label, yi_output, r)
        #------------------------------------------------------------------------
        self.r = r
        self.W_input = W_input
        self.n_iters = i

    #6、正向计算输出值
    def calResult(self, W_input, hiddens, X):
        y_output = 0
        bias = np.ones((self.n_samples, 1))
        for i in range(self.hiddenslayers+1):
            #print("第%d轮："%i)
            #print("===================")
            if i==0:
                #输入层计算：加入偏置项 -> 计算点乘项(输出项) -> 经过第一层隐藏层激活函数输出
                Xi = np.hstack((X, bias))
                #print("输入的格式",Xi.shape)
                Xi_dot = np.dot(Xi, W_input)
                #print("经过激活函数后的格式",Xi_dot.shape)
                hiddens[i].input = self.activeFuct(Xi_dot, hiddens[i].function)
            elif i==self.hiddenslayers:
                #输出层计算：加入偏置项 -> 计算点乘项(输出项) -> 经过输出层激活函数输出
                Yi = np.hstack((hiddens[i-1].input, bias))
                #print("输入的格式",Yi.shape)
                Yi_dot = np.dot(Yi, hiddens[i-1].w)
                hiddens[i-1].output = Yi_dot
                y_output = self.activeFuct(Yi_dot, self.Fuct_output)
                #print("经过激活函数后的格式",y_output.shape)
            else:
                #隐藏层计算：加入偏置项 -> 计算本层网络的输出项，并保存 -> 经过下层网络的激活函数输出
                Hi = np.hstack((hiddens[i-1].input, bias))
                #print("输入的格式",Hi.shape)
                Hi_dot = np.dot(Hi, hiddens[i-1].w)
                hiddens[i-1].output = Hi_dot
                hiddens[i].input = self.activeFuct(Hi_dot, hiddens[i].function)
                #print("经过激活函数后的格式",hiddens[i].input.shape)
        return y_output

    #7、误差反向传播进行优化
    def bwPropagation(self, W_input, hiddens, y, ypre, r):
        bias = np.ones((self.n_samples, 1))
        for i in range(self.hiddenslayers+1)[::-1]:
            if i==self.hiddenslayers:
                #最后一层隐藏层到输出层的权值更新：
                #计算误差 -> 计算输出层的梯度项 -> 更新权值
                error = ypre - y
                grdt_output = np.multiply(error, self.grdt_activeFuct(ypre, self.Fuct_output))
                hiddens[i-1].grdt = grdt_output
                Hi = np.hstack((hiddens[i-1].input, bias))
                w0 = hiddens[i-1].w.copy()
                hiddens[i-1].w = hiddens[i-1].w - r*np.dot(Hi.T, grdt_output)
            elif i==0:
                #输入层到第一层隐藏层的权值更新：
                ##计算误差 -> 计算上一层网络到这层网络的梯度项 -> 更新权值
                grdt_hidden = np.multiply(np.dot(hiddens[i].grdt, w0[:-1,:].T), \
                                          self.grdt_activeFuct(hiddens[i].input, hiddens[i].function))
                Xi = np.hstack((self.trainSet, bias))
                W_input = W_input - r*np.dot(Xi.T, grdt_hidden) 
            else:
                #这层隐藏层到下一层隐藏层的权值更新：
                #计算误差 -> 计算上一层网络到这层网络的梯度项 -> 更新权值
                grdt_hidden = np.multiply(np.dot(hiddens[i].grdt, w0[:-1,:].T), \
                                          self.grdt_activeFuct(hiddens[i].input, hiddens[i].function))
                hiddens[i-1].grdt = grdt_hidden
                Hi = np.hstack((hiddens[i-1].input, bias))
                w0 = hiddens[i-1].w.copy()
                hiddens[i-1].w = hiddens[i-1].w - r*np.dot(Hi.T, grdt_hidden)  
        return W_input